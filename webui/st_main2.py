from langchain_core.messages.base import BaseMessage
from src.config import hp
from openai import OpenAI
import streamlit as st
import re, json, pprint, os

st.title("MSDSğŸ§ªCHAT")

# å¯¼å…¥æ¨¡å‹å˜é‡
if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = hp.siliconflow_chat_model


# è®¾ç½®æ¶ˆæ¯å†å²è®°å½•æ–‡ä»¶çš„è·¯å¾„
HISTORY_FILE = "./webui/st_main2_chat_history.json"

# å¦‚æœèŠå¤©å†å²æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶
if not os.path.exists(HISTORY_FILE):
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump([], f, ensure_ascii=False)

# ä¿å­˜èŠå¤©å†å²åˆ°æ–‡ä»¶
def save_chat_history(messages):
    """
    å°†èŠå¤©å†å²ä¿å­˜åˆ°æ–‡ä»¶
    å‚æ•°:
        messages: åŒ…å«æ‰€æœ‰èŠå¤©æ¶ˆæ¯çš„åˆ—è¡¨
    """
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(messages, f, ensure_ascii=False)

# å®šä¹‰è¿”å›èŠå¤©å†å²çš„å‡½æ•°
def load_chat_history():
    """
    ä»æ–‡ä»¶åŠ è½½èŠå¤©å†å²
    è¿”å›:
        list: åŒ…å«èŠå¤©è®°å½•çš„åˆ—è¡¨ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨è¿”å›ç©ºåˆ—è¡¨
    """
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []


# å°†èŠå¤©å†å²åŠ è½½åˆ°session_stateä¸­
if "messages" not in st.session_state:
    st.session_state.messages = load_chat_history()

# å°†session_state.messagesä¸­çš„æ¶ˆæ¯æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š   
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# æ·»åŠ agentæ¨¡å¼å¼€å…³
use_agent = st.sidebar.toggle('å¯ç”¨Agentæ¨¡å¼', value=False)
# æ·»åŠ æ€è€ƒæ¨¡å¼å¼€å…³
think_mode = st.sidebar.toggle('å¯ç”¨æ€è€ƒæ¨¡å¼', value=True)

# æ·»åŠ æ¸…é™¤å†å²æ¶ˆæ¯æŒ‰é’®
if st.sidebar.button("æ¸…é™¤èŠå¤©å†å²"):
    st.session_state.messages = []
    save_chat_history([])
    st.experimental_rerun()


# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥MSDSç›¸å…³é—®é¢˜"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        thinking_placeholder = st.empty()
        full_response = ""
        thinking_content = ""
        
        if use_agent:
            # è°ƒç”¨agent.pyçš„åŠŸèƒ½
            from src.core.agent import graph

            out = graph.invoke(
                {
                    "messages": [
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„é—®ç­”æœºå™¨äººï¼Œä½ æ‹¥æœ‰å¾ˆå¤šå·¥å…·å¯ä»¥ä½¿ç”¨ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„æé—®æ¥å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ï¼Œè‡ªä¸»è¿›è¡Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¿…è¦æ—¶å¯ä»¥è°ƒç”¨å·¥å…·è·å–é¢å¤–çš„ä¿¡æ¯ã€‚",
                        },
                        {
                            "role": "human",
                            "content": prompt,
                        },
                    ]
                },
                config={"configurable": {"thread_id": 42}},
            )

            # æ·»åŠ è°ƒè¯•æ‰“å°
            print(f"Debug Output - outå˜é‡å†…å®¹ï¼š{out}")
            # åºåˆ—åŒ–æ¶ˆæ¯
            serializable = {
                "messages": [
                    {
                    "type": type(m).__name__,
                    "content": m.content
                    }
                for m in out["messages"]
            ]
            }
            # æ‰“å°åºåˆ—åŒ–åçš„æ¶ˆæ¯
            pprint.pprint(serializable)

            # å¤„ç†è¿”å›æ¶ˆæ¯
            if isinstance(out["messages"][0], BaseMessage):
                full_response = out["messages"][-1].content
            else:
                full_response = json.dumps(out["messages"][-1], ensure_ascii=False, indent=2)
        else:
            # æ™®é€šæ¨¡å¼ï¼Œç›´æ¥è°ƒç”¨API
            # åˆ›å»ºç©ºçš„å ä½ç¬¦ç”¨äºæµå¼è¾“å‡º
            if think_mode:
                # å¦‚æœå¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œä½¿ç”¨expanderæ¥æ˜¾ç¤ºæ€è€ƒå†…å®¹
                thinking_placeholder = st.expander("æ€è€ƒå†…å®¹", expanded=True).empty() # ä½¿ç”¨ empty() åˆ›å»ºå ä½ç¬¦
                message_placeholder = st.empty()
            else:
                # å¦‚æœä¸å¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œç›´æ¥ä½¿ç”¨ç©ºçš„å ä½ç¬¦
                thinking_placeholder = st.empty()

            client = OpenAI(
                base_url=hp.siliconflow_base_url,
            )

            # æ ¹æ®æ€è€ƒæ¨¡å¼æ·»åŠ å‰ç¼€
            processed_prompt = f"/think {prompt}" if think_mode else f"/nothink {prompt}"

            # è°ƒç”¨OpenAI APIè¿›è¡Œæµå¼å“åº”
            stream = client.chat.completions.create(
                model=st.session_state["openai_model"],
                messages=[
                    {"role": m["role"], "content": processed_prompt if m is st.session_state.messages[-1] else m["content"]}
                    for m in st.session_state.messages
                ],
                stream=True,
            )
            
            # æ”¶é›†å®Œæ•´çš„å“åº”
            if think_mode:
                for chunk in stream:
                    delta = chunk.choices[0].delta
                    if delta.reasoning_content is not None :
                        # æ›´æ–°æ€è€ƒå†…å®¹
                        thinking_content += delta.reasoning_content
                        # ä½¿ç”¨containeræ¥æ›´æ–°å†…å®¹ï¼Œé¿å…é‡å¤
                        thinking_placeholder.markdown(thinking_content + "â–Œ")
                    elif delta.content is not None:
                        # æ›´æ–°å›ç­”å†…å®¹
                        full_response += delta.content
                        message_placeholder.markdown(full_response + "â–Œ")
            elif not think_mode:
                for chunk in stream:
                    delta = chunk.choices[0].delta
                    if delta.content is not None:
                        # æ›´æ–°å›ç­”å†…å®¹
                        full_response += delta.content
                        message_placeholder.markdown(full_response + "â–Œ")
        
        st.session_state.messages.append({"role": "assistant", "content": full_response})
        save_chat_history(st.session_state.messages)